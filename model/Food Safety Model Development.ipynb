{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c82080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dill\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea2133b",
   "metadata": {},
   "source": [
    "# Food Safety Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ef0d1",
   "metadata": {},
   "source": [
    "This notebook details the development of a model to predict restaurant food safety from Google Maps reviews. The model is trained and validated using the [NYC Department of Health and Mental Hygiene restaurant inspection data](https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a29e90",
   "metadata": {},
   "source": [
    "## Inspection data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7a205",
   "metadata": {},
   "source": [
    "NYC DOHMH restaurant inspection data is catalogued in a large table with a lot of repeating information. To make life easier, I first reorganize the data into three tables:\n",
    "- `restaurant_data`\n",
    "    - Information about restaurants, including location and cuisine type\n",
    "    - columns: `CAMIS`, `DBA`, `BORO`, `CUISINE DESCRIPTION`, `Latitude`, `Longitude`\n",
    "- `review_codes`\n",
    "    - Information pertaining to violation categories and violation severity (critical or non-critical)\n",
    "    - columns: `VIOLATION CODE`, `CRITICAL FLAG`, `VIOLATION DESCRIPTION`\n",
    "- `inspection_data`\n",
    "    - Aggregated information from each individual restaurant inspection, including a list of violations and the restaurant grade\n",
    "    - columns: `CAMIS`, `INSPECTION DATE`, `VIOLATION CODE`, `SCORE`, `GRADE`\n",
    "    \n",
    "Cleaned code is saved in files `data/restaurant_data.csv`, `data/violation_codes.csv`, and `data/inspection_data.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff98005",
   "metadata": {},
   "source": [
    " ### Load data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOHMH_df = pd.read_csv(\n",
    "    'data/raw_data/DOHMH_New_York_City_Restaurant_Inspection_Results.csv',\n",
    "    # index_col = 'CAMIS',\n",
    "    usecols = ['CAMIS', 'DBA', 'BORO', 'CUISINE DESCRIPTION',\n",
    "                'INSPECTION DATE', 'VIOLATION CODE',\n",
    "                'VIOLATION DESCRIPTION', 'CRITICAL FLAG',\n",
    "                'SCORE', 'GRADE', 'Latitude', 'Longitude'],\n",
    "    parse_dates = ['INSPECTION DATE'],\n",
    "    dtype = {'CAMIS': 'str',\n",
    "             'DBA': 'str',\n",
    "             'BORO': 'str',\n",
    "             'CUISINE DESCRIPTION': 'category',\n",
    "             'SCORE': 'Int64',\n",
    "             'GRADE': 'str'}\n",
    ")\n",
    "\n",
    "# DOHMH_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5426ce2",
   "metadata": {},
   "source": [
    "#### `restaurant_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_data = DOHMH_df[[\n",
    "    'CAMIS', 'DBA', 'BORO', 'CUISINE DESCRIPTION', \n",
    "    'Latitude', 'Longitude'\n",
    "]].drop_duplicates()\n",
    "\n",
    "restaurant_data.to_pickle('data/restaurant_data.pkl') #, index=False)\n",
    "# restaurant_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0145635",
   "metadata": {},
   "source": [
    "#### `review_codes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc11475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review code key\n",
    "violation_codes = DOHMH_df.loc[DOHMH_df['VIOLATION CODE'].notna(), \n",
    "    ['VIOLATION CODE', 'CRITICAL FLAG', 'VIOLATION DESCRIPTION']\n",
    "].drop_duplicates(ignore_index = True)\n",
    "\n",
    "violation_codes.to_pickle('data/violation_codes.pkl')#, index=False)\n",
    "# violation_codes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900596ac",
   "metadata": {},
   "source": [
    "####  `inspection_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dfb9b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Inspection results - list of violation codes, score, \n",
    "# and grade for each CAMIS, INSPECTION DATE combo\n",
    "\n",
    "def get_score(x):\n",
    "    if x.min() == x.max():\n",
    "        return x.min()\n",
    "    if x.isna().all():\n",
    "        return pd.NA\n",
    "    \n",
    "    return x.mode()\n",
    "\n",
    "\n",
    "def get_grade(x):    \n",
    "    g0 = x.iloc[0]\n",
    "    for g in x.iloc[1:]:\n",
    "        if g != g0 and not (pd.isna(g) and pd.isna(g0)):\n",
    "            if pd.isna(g):\n",
    "                continue\n",
    "            elif pd.isna(g0):\n",
    "                g0 = g\n",
    "            else:\n",
    "                raise ValueError(f'Grades disagree: {g} {g0}')\n",
    "    return g0\n",
    "\n",
    "\n",
    "inspection_data = (\n",
    "    DOHMH_df[\n",
    "        ['CAMIS', 'INSPECTION DATE', 'VIOLATION CODE', 'SCORE', 'GRADE']\n",
    "    ]\n",
    "    .groupby(['CAMIS', 'INSPECTION DATE'], as_index=False)\n",
    "    .agg({\n",
    "        'VIOLATION CODE' : lambda x: list(x),\n",
    "        'SCORE' : lambda x: pd.Series.mode(x, dropna=False).max(),\n",
    "        'GRADE': lambda x: pd.Series.mode(x),\n",
    "    })\n",
    ")\n",
    "\n",
    "inspection_data.to_pickle('data/inspection_data.pkl')#, index=False)\n",
    "# inspection_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45380268",
   "metadata": {},
   "source": [
    "## Google Maps review data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87862a82",
   "metadata": {},
   "source": [
    "The data from Google Maps was downloaded in json format in several separate files. I want to reorganize this into two separate Pandas DataFrames covering all of the data:\n",
    "- `maps_rest_data`\n",
    "    - restaurant details\n",
    "    - `CAMIS`, name, price level, rating, and categorical type\n",
    "- `maps_review_data`\n",
    "    - review details\n",
    "    - `CAMIS`, author name, language, rating, text, time\n",
    "    \n",
    "The resulting data frames are saved in `data/maps_rest_data.csv` and `data/maps_review_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efdf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "maps_data_files = sorted(glob.glob('data/raw_data/gmaps_restaurant_data_*.pkd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_rest_subframes = []\n",
    "maps_review_subframes = []\n",
    "\n",
    "for j, filename in enumerate(maps_data_files):\n",
    "    f = open(filename, 'rb')\n",
    "    maps_data = dill.load(f)\n",
    "    \n",
    "    # Get restaurant details, clean up column names\n",
    "    maps_rest_subframes.append(\n",
    "        pd.json_normalize(maps_data)[\n",
    "            ['CAMIS', 'result.name', 'result.price_level', 'result.rating', 'result.types']\n",
    "        ].rename(columns = {\n",
    "            'result.name':'name',\n",
    "            'result.price_level':'price_level',\n",
    "            'result.rating':'rating',\n",
    "            'result.types':'types'\n",
    "        })\n",
    "        .astype({'CAMIS':'str'})\n",
    "    )\n",
    "\n",
    "    # If restaurant has no reviews, create reviews field as an empty list\n",
    "    # This avoids errors with pd.json_normalize\n",
    "    for entry in maps_data:\n",
    "        if not 'reviews' in entry['result']:\n",
    "            entry['result']['reviews'] = []\n",
    "    \n",
    "    # Get review details, remove repetitive/unneeded columns\n",
    "    maps_review_subframes.append(\n",
    "        pd.json_normalize(\n",
    "            maps_data, \n",
    "            ['result', 'reviews'], 'CAMIS'\n",
    "        ).drop(columns=[\n",
    "            'author_url', \n",
    "            'profile_photo_url', \n",
    "            'relative_time_description'])\n",
    "    )\n",
    "\n",
    "# Combine into single dataframe containing all restaurants\n",
    "maps_rest_data = pd.concat(maps_rest_subframes)\n",
    "maps_review_data = pd.concat(maps_review_subframes)\n",
    "\n",
    "# Save\n",
    "maps_rest_data.to_pickle('data/maps_rest_data.pkl')#, index=False)\n",
    "maps_review_data.to_pickle('data/maps_review_data.pkl')#, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f69971",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c194fbea",
   "metadata": {},
   "source": [
    "Use this to load data that has already been cleaned/organized and saved to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_data = pd.read_pickle('data/restaurant_data.pkl')\n",
    "violation_codes = pd.read_pickle('data/violation_codes.pkl')\n",
    "inspection_data = pd.read_pickle('data/inspection_data.pkl')\n",
    "\n",
    "maps_rest_data = pd.read_pickle('data/maps_rest_data.pkl') #, dtype={'CAMIS':'str'})\n",
    "maps_review_data = pd.read_pickle('data/maps_review_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps_review_data.head(10)\n",
    "inspection_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6de194",
   "metadata": {},
   "source": [
    "# A note on inspection data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11285b2f",
   "metadata": {},
   "source": [
    "Multiple inspections were performed at most restaurants. Data goes back to 2016. However, the limited reviews available from Google Maps (five maximum per restaurant) are mostly in 2021. See the below histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1dbbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# Plot histograms of inspection and review dates\n",
    "# ===============================================\n",
    "\n",
    "# Convert inspection date to datetime\n",
    "inspection_data['INSPECTION DATE'] = pd.to_datetime(inspection_data['INSPECTION DATE'])\n",
    "\n",
    "# Limit to latest inspections only\n",
    "idx = inspection_data.groupby(['CAMIS'])['INSPECTION DATE'].transform(max) == inspection_data['INSPECTION DATE']\n",
    "latest_inspections = inspection_data[idx]\n",
    "\n",
    "# Convert Google Maps review dates to datetime\n",
    "maps_review_data['time'] = pd.to_datetime(maps_review_data['time'], unit='s')\n",
    "\n",
    "\n",
    "# Plot histograms of years of all inspections, latest inspections, and Google Maps reviews\n",
    "plt.subplot(1,3,1)\n",
    "sns.histplot(\n",
    "    x = inspection_data.loc[inspection_data['INSPECTION DATE']>'1901','INSPECTION DATE'].dt.year,\n",
    "    stat = 'count',\n",
    "    binwidth = 1\n",
    ") #.hist()\n",
    "plt.xlim(2015,2022)\n",
    "# plt.ylim(0,1)\n",
    "plt.xticks([2015,2020])\n",
    "plt.title('All inspections')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.histplot(\n",
    "    x = latest_inspections.loc[latest_inspections['INSPECTION DATE']>'1901','INSPECTION DATE'].dt.year,\n",
    "    stat = 'count',\n",
    "    binwidth = 1\n",
    ") #.hist()\n",
    "plt.xlim(2015,2022)\n",
    "# plt.ylim(0,1)\n",
    "plt.xticks([2015,2020])\n",
    "plt.title('Latest inspections')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.histplot(\n",
    "    x = maps_review_data['time'].dt.year.to_list(),\n",
    "    stat = 'count',\n",
    "    binwidth = 1\n",
    ") #.hist()\n",
    "plt.xlim(2015,2022)\n",
    "# plt.ylim(0,1)\n",
    "plt.xticks([2015,2020])\n",
    "plt.title('Maps reviews')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81331bd1",
   "metadata": {},
   "source": [
    "From here on, I assume a restaurant's latest inspection results are the best estimate of its current health code compliance as well as the best estimate of the restaurant's cleanliness when most of its available Google Maps reviews were written."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ebb90",
   "metadata": {},
   "source": [
    "# Model development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2434181",
   "metadata": {},
   "source": [
    "## Predicting restaurant health inspection scores based on restaurant data (no review text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c94cca",
   "metadata": {},
   "source": [
    "First, I want to explore the utility of general restaurant data from Google Maps for predicting health inspection scores without the use of the review text itself. In this section, I examine correlations of `star_rating`, `price_level`, and `types` (establishment category) data with inspection scores.\n",
    "\n",
    "First, join tables, do some type conversion, and compute the grade associated with inspection scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8346ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join restaurant data with the inspection data\n",
    "joined = maps_rest_data.merge(latest_inspections, on='CAMIS')\n",
    "\n",
    "# Convert to float to avoid errors with seaborn plotting\n",
    "joined = joined.astype({'SCORE':'float64'})\n",
    "\n",
    "# Determine the associated NYC DOHMH grade for the restaurant's latest inspection score\n",
    "joined['SCOREGRADE'] = pd.cut(\n",
    "    joined['SCORE'],\n",
    "    bins = [-1, 14, 28, 200],\n",
    "    labels = ['A', 'B', 'C']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a88b11",
   "metadata": {},
   "source": [
    "### By star rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f36e71",
   "metadata": {},
   "source": [
    "First, star rating. There is a slight negative correlation between star rating and inspection scores, meaning that restaurants with higher star ratings have better inspection results (low scores are good). This correlation is, however, very weak - star rating does not fully explain the variation in inspection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a7a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Linear model of correlation between Maps rating and inspection score\n",
    "rating_model = Ridge()\n",
    "rating_model.fit(joined[joined['rating'] > 3.0].dropna()[['rating']], joined[joined['rating'] > 3.0].dropna()['SCORE'])\n",
    "\n",
    "\n",
    "# Plot prediction with the data\n",
    "grouped = joined.groupby('SCOREGRADE')\n",
    "for name, group in grouped:\n",
    "    plt.scatter(group['rating'], group['SCORE'], alpha=0.05, label=f'{name} grade', s=20)\n",
    "\n",
    "X = np.arange(1.0, 6.0, 1.0)\n",
    "sns.lineplot(x=X, y=rating_model.predict(X.reshape(-1,1)), color='k', lw=2, label='prediction')\n",
    "\n",
    "leg = plt.legend()\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "\n",
    "plt.xlabel('Google Maps rating')\n",
    "plt.ylabel('Inspection Score')\n",
    "plt.title(\n",
    "    'Weak negative correlation between review rating and inspection score, coef = {:.2}, score = {:.3}'.format(\n",
    "    rating_model.coef_[0],\n",
    "    rating_model.score(joined.dropna()[['rating']], joined.dropna()['SCORE'])\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8185abf",
   "metadata": {},
   "source": [
    "### By price level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f260ef38",
   "metadata": {},
   "source": [
    "Again, the correlation between price level and inspection scores is very weak. More expensive restaurants tend to achieve better health inspection results, however the variation in inspection results within each price range is greater than the variation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model of price level and inspection score correlation\n",
    "pricelevel_model = Ridge()\n",
    "pricelevel_model.fit(joined.dropna()[['price_level']], joined.dropna()['SCORE'])\n",
    "\n",
    "\n",
    "# Plot prediction along with input data\n",
    "sns.violinplot(x=joined['price_level'], y=joined['SCORE'], palette=\"Set2\");\n",
    "\n",
    "X = np.arange(1.0, 5.0, 1.0)\n",
    "\n",
    "sns.lineplot(X-1, pricelevel_model.predict(X.reshape(-1,1)), color='k', lw=2, label='prediction')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\n",
    "    'Weak positive correlation of inspection score with restaurant price level, coef = {:.2}, score = {:.3}'.format(\n",
    "    pricelevel_model.coef_[0],\n",
    "    pricelevel_model.score(joined.dropna()[['rating']], joined.dropna()['SCORE'])\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7ed34",
   "metadata": {},
   "source": [
    "## By establishment category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df90d8a",
   "metadata": {},
   "source": [
    "Correlations between establishment categories and inspection results are stronger, with, e.g., supermarkets and bowling alleys performing relatively poorly and book stores and shopping malls performing relatively well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0480e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, need to convert the list of establishment categories to a string\n",
    "# which will be fed to the CountVectorizer\n",
    "def join_types(type_series):\n",
    "    joinedtypes = []\n",
    "    for l in type_series:\n",
    "        try:\n",
    "            joinedtypes.append(' '.join(l))\n",
    "        except:\n",
    "            joinedtypes.append([])\n",
    "    \n",
    "    return joinedtypes\n",
    "\n",
    "joined['joinedtypes'] = join_types(joined['types'].to_list())\n",
    "# joined.head(5)\n",
    "\n",
    "\n",
    "# Build a linear model using CountVectorizer and Ridge regressor\n",
    "category_model = Pipeline([\n",
    "    ('count_types', CountVectorizer()),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "category_model.fit(joined.dropna()['joinedtypes'], joined.dropna()['SCORE'])\n",
    "\n",
    "# Save dictionary of word tokens\n",
    "typemap = {}\n",
    "for key, value in category_model['count_types'].vocabulary_.items():\n",
    "    typemap[value] = key\n",
    "\n",
    "typecoefs = []\n",
    "for j, coef in enumerate(category_model[-1].coef_):\n",
    "    typecoefs.append((coef, typemap[j]))\n",
    "    \n",
    "typecoefs.sort()\n",
    "\n",
    "# Plot correlation coefficients\n",
    "fig=plt.figure(figsize=(6,12), dpi= 100)\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "corr_coefs = [t[0] for t in typecoefs]\n",
    "categories = [t[1] for t in typecoefs]\n",
    "sns.set_color_codes('pastel')\n",
    "ax = sns.barplot(x=corr_coefs, y = categories, color='b')\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "plt.title('Correlation of establishment category with inspection scores, model score = {:.3}'\n",
    "          .format(category_model.score(joined.dropna()['joinedtypes'], joined.dropna()['SCORE']))\n",
    "         );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd4d03",
   "metadata": {},
   "source": [
    "## Full restaurant data model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc54af",
   "metadata": {},
   "source": [
    "Now, we combine star rating, price level, and establishment category into a single model to predict inspection results. The model still leaves a lot of the variation in inspection results unexplained, with a final score of $R^2 = 0.00921$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4fff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full model based on all restaurant data\n",
    "\n",
    "def join_types(type_series):\n",
    "    joinedtypes = []\n",
    "    for l in type_series:\n",
    "        try:\n",
    "            joinedtypes.append(' '.join(l))\n",
    "        except:\n",
    "            joinedtypes.append([])\n",
    "    \n",
    "    return joinedtypes\n",
    "\n",
    "joined['joinedtypes'] = join_types(joined['types'].to_list())\n",
    "joined.head(5)\n",
    "\n",
    "\n",
    "encode_categories = ColumnTransformer([\n",
    "    ('count_types', CountVectorizer(), 'joinedtypes'),\n",
    "    ('pass', 'passthrough', ['price_level', 'rating']),\n",
    "])\n",
    "\n",
    "\n",
    "maps_rest_data_model = Pipeline([\n",
    "    ('category_encode', encode_categories),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "maps_rest_data_model.fit(joined.dropna()[['price_level', 'rating', 'joinedtypes']], joined.dropna()['SCORE'])\n",
    "print(\n",
    "    'Full restaurant data model score: {:.3}'\n",
    "    .format(\n",
    "        maps_rest_data_model\n",
    "        .score(\n",
    "            joined.dropna()[['price_level', 'rating', 'joinedtypes']],\n",
    "            joined.dropna()['SCORE']\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd0628",
   "metadata": {},
   "source": [
    "# Predicting inspection results from Google Maps review text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35a2b8",
   "metadata": {},
   "source": [
    "To attempt to improve the ability of the model to predict health inspection scores, we can process the text of the reviews to find relations between specific words or phrases and inspection performance. This model performs similarly to the model above.\n",
    "\n",
    "First, we combine all reviews for a particular restaurant into a single corpus of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_review_text = (\n",
    "    maps_review_data[['CAMIS','text']]\n",
    "    .astype({'text':str, 'CAMIS':str})\n",
    "    .groupby('CAMIS', as_index=False)\n",
    "    .agg({\n",
    "        'text': lambda x: ' '.join(x)\n",
    "    })\n",
    ")\n",
    "\n",
    "text_model_df = maps_review_text.merge(latest_inspections[['CAMIS', 'SCORE']], on='CAMIS').dropna()\n",
    "# text_model_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988cfe93",
   "metadata": {},
   "source": [
    "Next, we build the model pipeline. I'm using CountVectorizer to tokenize corpus words and bigrams, only allowing words consisting of letters, followed by a TfidfTransformer and the Ridge regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewtext_model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(\n",
    "        ngram_range=(1,2), \n",
    "        stop_words='english',\n",
    "#         token_pattern=r'\\b[a-zA-Z][a-zA-Z]+\\b', # match only words (no numbers)\n",
    "    )),\n",
    "    ('tfidf_transform', TfidfTransformer()),\n",
    "    ('regressor', Ridge()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d3020",
   "metadata": {},
   "source": [
    "The feature space of the model is large compared to the object space, so we need to be careful about overfitting. Let's take a train/test split so we can fairly evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f5192",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_model_df['text'], \n",
    "    text_model_df['SCORE'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a91db",
   "metadata": {},
   "source": [
    "Now we can train the model and perform a grid search for cross validation and parameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de49cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gs = GridSearchCV(\n",
    "    reviewtext_model, \n",
    "    param_grid = {\n",
    "        'vectorizer__min_df':[10, 15], \n",
    "#         'vectorizer__max_df':[10000],#, 2000, 5000],\n",
    "#         'vectorizer__stop_words':[STOP_WORDS],\n",
    "        'regressor__alpha':[1.0, 10.0, 50.0],\n",
    "    },\n",
    "    n_jobs = -1,\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d43bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model score on training data: {:.3}'.format(gs.score(X_train, y_train)))\n",
    "print('Model score on test data: {:.3}'.format(gs.score(X_test, y_test)))\n",
    "print('\\n')\n",
    "print(f'Grid search best parameters: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d234946",
   "metadata": {},
   "source": [
    "View some example predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07208355",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = gs.predict(text_model_df['text'])\n",
    "\n",
    "with_predict = text_model_df.copy()\n",
    "with_predict['prediction'] = prediction\n",
    "\n",
    "with_predict.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629467b8",
   "metadata": {},
   "source": [
    "# Combined model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522e8e4b",
   "metadata": {},
   "source": [
    "Now, I combine all of the above models into a single model to attempt to get the best possible performance I can acheive. This model is saved in order to be used in the Hugo web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c601920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# SET UP THE MODEL PIPELINE\n",
    "# ======================================\n",
    "reviewtext_pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(\n",
    "        ngram_range=(1,2), \n",
    "        stop_words='english',\n",
    "        min_df=5,\n",
    "    )),\n",
    "    ('tfidf_transform', TfidfTransformer()),\n",
    "])\n",
    "submodel_transform = ColumnTransformer([\n",
    "    ('count_types', CountVectorizer(), 'joinedtypes'),\n",
    "    ('pass', 'passthrough', ['price_level', 'rating']),\n",
    "    ('reviewtext_data', reviewtext_pipe, 'text'),\n",
    "])\n",
    "full_model = Pipeline([\n",
    "    ('submodels', submodel_transform),\n",
    "    ('normalizer', Normalizer()),\n",
    "    ('regressor', RidgeCV()), # Use RidgeCV for cross-validation\n",
    "])\n",
    "\n",
    "# =======================================\n",
    "# PREPROCESSING\n",
    "# =======================================\n",
    "# Convert inspection date to datetime\n",
    "inspection_data['INSPECTION DATE'] = pd.to_datetime(inspection_data['INSPECTION DATE'])\n",
    "\n",
    "# Limit to latest inspections only\n",
    "idx = inspection_data.groupby(['CAMIS'])['INSPECTION DATE'].transform(max) == inspection_data['INSPECTION DATE']\n",
    "latest_inspections = inspection_data[idx]\n",
    "\n",
    "# Restaurant data preprocessing\n",
    "# Join restaurant data with the inspection data\n",
    "joined = maps_rest_data.merge(latest_inspections, on='CAMIS')\n",
    "\n",
    "# Convert to float to avoid errors with seaborn plotting\n",
    "joined = joined.astype({'SCORE':'float64'})\n",
    "\n",
    "# Category model preprocessing\n",
    "def join_types(type_series):\n",
    "    joinedtypes = []\n",
    "    for l in type_series:\n",
    "        try:\n",
    "            joinedtypes.append(' '.join(l))\n",
    "        except:\n",
    "            joinedtypes.append([])    \n",
    "    return joinedtypes\n",
    "joined['joinedtypes'] = join_types(joined['types'].to_list())\n",
    "\n",
    "# Review text preprocessing\n",
    "maps_review_text = (\n",
    "    maps_review_data[['CAMIS','text']]\n",
    "    .astype({'text':str, 'CAMIS':str})\n",
    "    .groupby('CAMIS', as_index=False)\n",
    "    .agg({\n",
    "        'text': lambda x: ' '.join(x)\n",
    "    })\n",
    ")\n",
    "text_model_df = maps_review_text.merge(latest_inspections[['CAMIS']], on='CAMIS').dropna()\n",
    "\n",
    "# Combine preprocessed data frames\n",
    "full_model_df = joined.merge(text_model_df, on='CAMIS')\n",
    "full_model_df[['rating', 'price_level', 'joinedtypes', 'text']].head(10)\n",
    "full_model_df = full_model_df.dropna()\n",
    "\n",
    "# =======================================\n",
    "# FIT AND EVALUATE MODEL\n",
    "# =======================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    full_model_df[['rating', 'price_level', 'joinedtypes', 'text']], \n",
    "    full_model_df['SCORE'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "full_model.fit(X_train, y_train)\n",
    "print('Training data score: {:.3}'.format(full_model.score(X_train, y_train)))\n",
    "print('Testing data score: {:.3}'.format(full_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84093b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# SAVE MODEL\n",
    "# =======================================\n",
    "# from joblib import dump\n",
    "\n",
    "# dump(full_model, 'Hugo_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e3bcf",
   "metadata": {},
   "source": [
    "# Other thoughts..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27596788",
   "metadata": {},
   "source": [
    "## Negative words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575a781",
   "metadata": {},
   "source": [
    "I want to find the words and phrases most uniquely associated with restaurants that perform poorly on health inspections so that I can highlight occurrences of those words or phrases in Google Maps reviews and display them in the web app. I limit the data to restaurants with inspection scores corresponding to A (0-13) or C (28+) grades and use a Naive Bayes classifier to find words in review text most associated with those grades.\n",
    "\n",
    "This model is unsuccessful in the sense that the words identified as negative words are not indicative of any particular poor practices witnessed at restaurants - often they are simply food words or even a person's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720bc59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_inspections = text_model_df.copy()\n",
    "\n",
    "polar_inspections['SCOREGRADE'] = pd.cut(\n",
    "    polar_inspections['SCORE'],\n",
    "    bins = [-1, 14, 28, 200],\n",
    "    labels = ['A', 'B', 'C']\n",
    ")\n",
    "\n",
    "polar_inspections = polar_inspections[polar_inspections['SCOREGRADE'].isin(['A', 'C'])]\n",
    "\n",
    "# polar_inspections.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22671d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "polar_model = Pipeline([\n",
    "#     ('extractor', TextExtractor()),\n",
    "    ('count_vectorizer', CountVectorizer(\n",
    "        stop_words='english', \n",
    "        ngram_range=(1,2),\n",
    "        min_df=6,\n",
    "    )), #n_features=30000, \n",
    "    ('tfidf_transform', TfidfTransformer()),\n",
    "    ('regressor', MultinomialNB()),\n",
    "])\n",
    "\n",
    "polar_model.fit(polar_inspections['text'], polar_inspections['SCOREGRADE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary of word tokens\n",
    "wordmap = {}\n",
    "for key, value in polar_model['count_vectorizer'].vocabulary_.items():\n",
    "    wordmap[value] = key\n",
    "\n",
    "# Get polarity of words\n",
    "polarity = polar_model[-1].feature_log_prob_\n",
    "\n",
    "# Get polar words\n",
    "negativity = polarity[1,:] - polarity[0,:]\n",
    "negative_words = [wordmap[i] for i in np.argsort(negativity)]\n",
    "print(negative_words[-100:])\n",
    "print(len(negative_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c0103",
   "metadata": {},
   "source": [
    "At first glance, the most negatively polar words don't appear particularly negative or associated with restaurant cleanliness in any way, but maybe we can still use them to help make predictions of a restaurant's inspection score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d8735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "polar_inspections['negative_count'] = polar_inspections['text'].str.count('|'.join(negative_words[-100:]))\n",
    "\n",
    "sns.violinplot(x=polar_inspections['negative_count'], y=polar_inspections['SCORE'], palette=\"Set3\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4504c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    polar_inspections[['negative_count']], \n",
    "    polar_inspections['SCORE'], \n",
    "    test_size=0.2, \n",
    "    random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ed020",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words_model = Ridge()\n",
    "negative_words_model.fit(X_train, y_train)\n",
    "\n",
    "print('Training data score: {:.3}'.format(negative_words_model.score(X_train, y_train)))\n",
    "print('Testing data score: {:.3}'.format(negative_words_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f0398",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bbd074",
   "metadata": {},
   "source": [
    "This code (most of which was early data exploration) is no longer central to the notebook. I may decide to bring some portion of it back later, so I'm keeping it here for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a229614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's get a first look at the variations in review ratings and inspection scores with restaurant price level\n",
    "# plt.subplot(1,2,1)\n",
    "# sns.violinplot(x=joined['price_level'], y=joined['rating'], palette=\"Set2\");\n",
    "# plt.subplot(1,2,2)\n",
    "# sns.violinplot(x=joined['price_level'], y=joined['SCORE'], palette=\"Set2\");\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4aed3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sns.histplot(\n",
    "#     data=joined.loc[joined['GRADE'].isin(['A', 'B', 'C']), ['rating', 'price_level','GRADE']].sort_values('GRADE'),\n",
    "#     x='GRADE',\n",
    "#     hue='price_level',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "# #     stat='density'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(1,3,1)\n",
    "# sns.histplot(\n",
    "#     data=joined.loc[joined['GRADE']=='A', ['rating', 'price_level','GRADE']].sort_values('GRADE'),\n",
    "#     x='GRADE',\n",
    "#     hue='price_level',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "#     stat='percent'\n",
    "# )\n",
    "# plt.subplot(1,3,2)\n",
    "# sns.histplot(\n",
    "#     data=joined.loc[joined['GRADE']=='B', ['rating', 'price_level','GRADE']].sort_values('GRADE'),\n",
    "#     x='GRADE',\n",
    "#     hue='price_level',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "#     stat='percent'\n",
    "# )\n",
    "# plt.subplot(1,3,3)\n",
    "# sns.histplot(\n",
    "#     data=joined.loc[joined['GRADE']=='C', ['rating', 'price_level','GRADE']].sort_values('GRADE'),\n",
    "#     x='GRADE',\n",
    "#     hue='price_level',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "#     stat='percent'\n",
    "# )\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_stars = joined\n",
    "# joined_stars['stars'] = joined['rating'].round(0)\n",
    "\n",
    "# axs = []\n",
    "# axs.append(plt.subplot(1,4,1))\n",
    "# sns.histplot(\n",
    "#     data=(\n",
    "#         joined\n",
    "#         .loc[(joined['price_level']==1) & joined['GRADE'].isin(['A','B','C']), ['stars', 'price_level','GRADE']]\n",
    "#         .sort_values('GRADE'))\n",
    "#     ,\n",
    "#     x='price_level',\n",
    "#     hue='GRADE',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "#     stat='percent'\n",
    "# )\n",
    "# plt.title('Price: \\$')\n",
    "\n",
    "# axs.append(plt.subplot(1,4,2))\n",
    "# sns.histplot(\n",
    "#     data=(\n",
    "#         joined\n",
    "#         .loc[(joined['price_level']==2) & joined['GRADE'].isin(['A','B','C']), ['stars', 'price_level','GRADE']]\n",
    "#         .sort_values('GRADE'))\n",
    "#     ,\n",
    "#     x='price_level',\n",
    "#     hue='GRADE',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "#     stat='percent'\n",
    "# )\n",
    "# plt.title('\\$\\$')\n",
    "\n",
    "# axs.append(plt.subplot(1,4,3))\n",
    "# sns.histplot(\n",
    "#     data=(\n",
    "#         joined\n",
    "#         .loc[(joined['price_level']==3) & joined['GRADE'].isin(['A','B','C']), ['stars', 'price_level','GRADE']]\n",
    "#         .sort_values('GRADE'))\n",
    "#     ,\n",
    "#     x='price_level',\n",
    "#     hue='GRADE',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "#     stat='percent'\n",
    "# )\n",
    "# plt.title('\\$\\$\\$')\n",
    "\n",
    "# axs.append(plt.subplot(1,4,4))\n",
    "# sns.histplot(\n",
    "#     data=(\n",
    "#         joined\n",
    "#         .loc[(joined['price_level']==4) & joined['GRADE'].isin(['A','B','C']), ['stars', 'price_level','GRADE']]\n",
    "#         .sort_values('GRADE'))\n",
    "#     ,\n",
    "#     x='price_level',\n",
    "#     hue='GRADE',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "#     stat='percent'\n",
    "# )\n",
    "# plt.title('\\$\\$\\$\\$')\n",
    "\n",
    "# for ax in axs[1:]:\n",
    "#     ax.set(ylabel=None)\n",
    "#     ax.set(yticks=[])\n",
    "#     ax.get_legend().remove()\n",
    "# for ax in axs:\n",
    "#     ax.set(xlabel=None)\n",
    "#     ax.set(xticks=[])\n",
    "    \n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd95f53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sns.histplot(\n",
    "#     data=(\n",
    "#         joined\n",
    "#         .loc[joined['GRADE'].isin(['A','B','C']), ['stars', 'price_level','GRADE']]\n",
    "#         .sort_values('GRADE'))\n",
    "#     ,\n",
    "#     x='price_level',\n",
    "#     palette='Set2',\n",
    "#     stat='count'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77816bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_stars = joined\n",
    "# joined_stars['stars'] = joined['rating'].round(0)\n",
    "\n",
    "# axs = []\n",
    "# axs.append(plt.subplot(1,5,1))\n",
    "# sns.histplot(\n",
    "#     data=(\n",
    "#         joined\n",
    "#         .loc[(joined['stars']==1) & joined['GRADE'].isin(['A','B','C']), ['stars', 'price_level','GRADE']]\n",
    "#         .sort_values('GRADE'))\n",
    "#     ,\n",
    "#     x='stars',\n",
    "#     hue='GRADE',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "#     stat='percent'\n",
    "# )\n",
    "# plt.title('One star')\n",
    "\n",
    "# axs.append(plt.subplot(1,5,2))\n",
    "# sns.histplot(\n",
    "#     data=(\n",
    "#         joined\n",
    "#         .loc[(joined['stars']==2) & joined['GRADE'].isin(['A','B','C']), ['stars', 'price_level','GRADE']]\n",
    "#         .sort_values('GRADE'))\n",
    "#     ,\n",
    "#     x='stars',\n",
    "#     hue='GRADE',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "#     stat='percent'\n",
    "# )\n",
    "# plt.title('Two stars')\n",
    "\n",
    "# axs.append(plt.subplot(1,5,3))\n",
    "# sns.histplot(\n",
    "#     data=(\n",
    "#         joined\n",
    "#         .loc[(joined['stars']==3) & joined['GRADE'].isin(['A','B','C']), ['stars', 'price_level','GRADE']]\n",
    "#         .sort_values('GRADE'))\n",
    "#     ,\n",
    "#     x='stars',\n",
    "#     hue='GRADE',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "#     stat='percent',\n",
    "# )\n",
    "# plt.title('Three stars')\n",
    "\n",
    "# axs.append(plt.subplot(1,5,4))\n",
    "# sns.histplot(\n",
    "#     data=(\n",
    "#         joined\n",
    "#         .loc[(joined['stars']==4) & joined['GRADE'].isin(['A','B','C']), ['stars', 'price_level','GRADE']]\n",
    "#         .sort_values('GRADE'))\n",
    "#     ,\n",
    "#     x='stars',\n",
    "#     hue='GRADE',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "#     stat='percent'\n",
    "# )\n",
    "# plt.title('Four stars')\n",
    "\n",
    "# axs.append(plt.subplot(1,5,5))\n",
    "# sns.histplot(\n",
    "#     data=(\n",
    "#         joined\n",
    "#         .loc[(joined['stars']==4) & joined['GRADE'].isin(['A','B','C']), ['stars', 'price_level','GRADE']]\n",
    "#         .sort_values('GRADE'))\n",
    "#     ,\n",
    "#     x='stars',\n",
    "#     hue='GRADE',\n",
    "#     multiple='stack',\n",
    "#     palette='Set2',\n",
    "#     stat='percent'\n",
    "# )\n",
    "# plt.title('Five stars')\n",
    "\n",
    "# for ax in axs[1:]:\n",
    "#     ax.set(ylabel=None)\n",
    "#     ax.set(yticks=[])\n",
    "# for ax in axs:\n",
    "#     ax.set(xlabel=None)\n",
    "#     ax.set(xticks=[])\n",
    "# for ax in axs[:-1]:\n",
    "#     ax.get_legend().remove()\n",
    "    \n",
    "\n",
    "# plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
